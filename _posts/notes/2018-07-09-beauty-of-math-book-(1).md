---
layout: post
title: "《数学之美》阅读笔记（一）"
author: Kevin9436
date: 2018-07-09 20:10
category: notes
---

上周在网上找数据处理的书籍，看到有人推荐这本《数学之美》吴军著，突然想起自己有这本书，便从书架上翻出来看。说起这本书，还是当年大哥留给我的，翻出这本书的时候和大哥相处的一些往事还历历在目，大哥曾给予了我莫大的帮助和关心，这里也祝愿大哥在USC毕业后顺利拿到好的offer，和菲神也是幸福快乐。  
这本书总共有31章，立个flag每天看一到两章，争取一个月内看完，半周或者一周整理一次笔记，大概就这样吧。  
****  

## 第一章 文字和语言vs数字和信息  
其实作者在第一章分析人类文字语言发展的过程中就提出了自然语言处理的宏观模型：  
> 信息源->(编码)->信道->(解码)->信息接受者  
这个模型将在之后的章节提到并深入解释。  
## 第二章 自然语言处理——从规则到统计  
核心思想：文法规则行不通，统计分析才靠谱  
## 第三章 统计语言模型  
### 1. 应用领域
机器翻译、语音识别、印刷体或手写体识别、拼写纠错、汉字输入和文献查询等  
### 2. 核心思想
一个句子是否合理，就看它的可能性大小如何。
### 3. 建模过程  
假设S代表一个句子，由一串特定顺序排列的词w<sub>1</sub>,w<sub>2</sub>,...,w<sub>n</sub>组成，则$ S=w_1,w_2,...,w_n $。  
P(S)=P(w<sub>1</sub>,w<sub>2</sub>,...,w<sub>n</sub>)=P(w<sub>1</sub>)·P(w<sub>2</sub>|w<sub>1</sub>)···P(w<sub>n</sub>|w<sub>1</sub>,w<sub>2</sub>,...,w<sub>n-1</sub>)	(3.1)  
$$ P(s)=P(w_1,w_2,{\rm ldot},w_n)=P(w_1)·P(w_2|w_1){\rm cdot}P(w_n|w_1,w_2,{\rm ldot},w_{n-1}) \tag{(3.1)} $$
> N-1阶马尔可夫假设：假设任一个词w<sub>i</sub>出现的概率只同它前面的N-1个词有关。即：  
> P(w<sub>i</sub>|w<sub>1</sub>,w<sub>2</sub>,...,w<sub>i-1</sub>)=P(w<sub>i</sub>|w<sub>i-N+1</sub>,w<sub>i-N+2</sub>,...,w<sub>i-1</sub>)	(3.2)  
> $$ P(w_i|w_1,w_2,{\rm ldot},w_{i-1})=P(w_i|w_{i-N+1},w_{i-N+2},{\rm ldot},w_{i-1}) \tag{(3.2)} $$
将公式(3.2)代入到公式(3.1)中就得到了N元模型。 实际应用中大多N=3，因为N元模型空间复杂度为O(|V|<sup>N</sup>)，时间复杂度为O(|V|<sup>N-1</sup>)，因此为了节省资源N一般很小。  
以二元模型为例：  
$$ P(w_i|w_{i-1})=\frac{P(w_{i-1},w_i)}{P(w_{i-1})} \approx \frac{#(w_{i-1},w_i)}{#(w_{i-1})} $$  
其中#(w<sub>i-1</sub>),w<sub>i</sub>)表示w<sub>i-1</sub>,w<sub>i</sub>这对词在统计文本中前后相邻出现的次数；#(w<sub>i</sub>)同理。
